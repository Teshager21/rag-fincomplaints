# ğŸ’¬ Financial Complaints RAG Chatbot

**Financial Complaints RAG Chatbot** is an AI-powered assistant that answers questions about financial customer complaints, based on real complaint narratives.

- Retrieves relevant complaint excerpts from a FAISS vector store
- Uses sentence-transformers embeddings
- Generates answers with an LLM (Flan-T5 Small)
- Displays sources for transparency
- Runs via either Streamlit or Gradio interfaces

---

## ğŸš€ Features

âœ… Retrieval-Augmented Generation (RAG) pipeline
âœ… Fast similarity search with FAISS
âœ… Supports CPU and GPU execution
âœ… Works in light or dark mode
âœ… Looks and feels like ChatGPT
âœ… Easy deployment via:
- Streamlit UI
- Gradio ChatInterface

---

## ğŸ› ï¸ Installation

1. **Clone the repo**

```bash
git clone https://github.com/YOUR_USERNAME/rag-fincomplaints.git
cd rag-fincomplaints
```

2. **Create a virtual environment**

```bash
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

> ğŸ’¡ If running on GPU, ensure your PyTorch and CUDA versions are compatible.

---

## ğŸ—ï¸ Vector Store

Your vector store lives under:

```
vector_store/faiss_index
```

- Ensure this folder exists and contains the saved FAISS index.
- If you need to build your index, modify `query_rag_pipeline.py` to index your complaint data.

---

## ğŸ’» Running the App

### Option 1 â€” Streamlit

Start the Streamlit app:

```bash
streamlit run app.py
```

Then open the provided URL (e.g. http://localhost:8501).

---

### Option 2 â€” Gradio

Start the Gradio app:

```bash
python3 app_gradio.py
```

Then visit [http://localhost:7860](http://localhost:7860)

> Youâ€™ll see a UI like this:
>
> *(Replace this with your real screenshot filename in your repo!)*

---

## ğŸ¤– Example Gradio Chatbot Usage

```python
import requests

# Example local Gradio call:
url = "http://127.0.0.1:7860/run/predict"
payload = {
  "data": ["What issues occur with BNPL products?", []]
}

r = requests.post(url, json=payload)
print(r.json())
```

---

## ğŸ§© Project Structure

```
â”œâ”€â”€ app.py                  # Streamlit app
â”œâ”€â”€ app_gradio.py           # Gradio app
â”œâ”€â”€ src/
â”‚   â””â”€â”€ rag/
â”‚       â””â”€â”€ query_rag_pipeline.py
â”œâ”€â”€ vector_store/
â”‚    â””â”€â”€ faiss_index/       # Saved FAISS index
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Tech Stack

- Python 3.12+
- LangChain
- sentence-transformers
- HuggingFace Transformers
- FAISS
- Streamlit
- Gradio

---

## âš ï¸ Troubleshooting

### CUDA errors

If you see errors like:

```
RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
```

- Ensure your GPU drivers and CUDA versions match PyTorch
- Run in CPU mode by forcing:

```python
device = -1
```

in `transformers.pipeline(...)`

Or set:

```python
model_kwargs={"device": "cpu"}
```

when loading HuggingFace embeddings.

---

## ğŸ“ License

MIT License

---

## ğŸ™Œ Credits

This chatbot was developed as part of a financial complaints analysis project for 10 Academy.

---

## â­ï¸ Future Improvements

- Switch to newer `langchain-huggingface` import
- Add streaming token-by-token output
- UI polish to perfectly mimic ChatGPT (light & dark themes)
- Cloud deployment (e.g. Hugging Face Spaces)
